{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 0.10 - Description\n",
    "- Changed _tree method_ parametor to _hist_, similar method to LGBM: increased speed and slightly better CV (0.386 --> 0.390)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import ml_metrics\n",
    "import string\n",
    "import nltk\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hides warnings - think it needs running after modules imported\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = 42  # random state for scoring consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train/train.csv\", index_col=\"PetID\")\n",
    "df_test = pd.read_csv(\"../input/test/test.csv\", index_col=\"PetID\")\n",
    "df_breeds = pd.read_csv(\"../input/breed_labels.csv\", index_col=\"BreedID\")\n",
    "df_colors = pd.read_csv(\"../input/color_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = df_colors['ColorID']\n",
    "breeds = df_breeds.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tfidf Vectorizer (Better BoW technique)\n",
    "- Extracts non-stopwords from descriptions\n",
    "- Applies weighting to text depending on commonalities\n",
    "- Weight is dependent on how many other words are shared and quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer for description, to return list of word tokens\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['malibu',\n",
       " ':',\n",
       " 'femal',\n",
       " ',',\n",
       " 'local',\n",
       " 'mix',\n",
       " ',',\n",
       " '4-5',\n",
       " 'month',\n",
       " ',',\n",
       " 'vaccin',\n",
       " 'and',\n",
       " 'spay',\n",
       " '.',\n",
       " 'strike',\n",
       " 'featur',\n",
       " 'with',\n",
       " 'fade',\n",
       " 'beig',\n",
       " 'fur',\n",
       " 'and',\n",
       " 'jet-yellow',\n",
       " 'eye',\n",
       " '.',\n",
       " 'natur',\n",
       " 'curiou',\n",
       " 'explor',\n",
       " ',',\n",
       " 'immedi',\n",
       " 'taken',\n",
       " 'to',\n",
       " 'human',\n",
       " 'interact',\n",
       " 'and',\n",
       " 'love',\n",
       " 'to',\n",
       " 'play',\n",
       " 'around',\n",
       " '.',\n",
       " 'come',\n",
       " 'and',\n",
       " 'meet',\n",
       " 'our',\n",
       " 'anim',\n",
       " 'for',\n",
       " 'adopt',\n",
       " 'at',\n",
       " 'selangor',\n",
       " '(',\n",
       " 'tue',\n",
       " '-',\n",
       " 'sun',\n",
       " ',',\n",
       " '10am',\n",
       " '-',\n",
       " '4pm',\n",
       " ')',\n",
       " 'and',\n",
       " 'fall',\n",
       " 'in',\n",
       " 'love',\n",
       " '!',\n",
       " 'www.spca.org.mi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of tokenized text\n",
    "text = df_test['Description'][3]\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malibu female local mix 45 months vaccinated and spayed striking features with faded beige fur and jetyellow eyes naturally curious explorer immediately taken to human interaction and loves to play around come and meet our animals for adoption at selangor tues  sun 10am  4pm and fall in love wwwspcaorgmy\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from text\n",
    "remove_punc = str.maketrans({key: None for key in string.punctuation})\n",
    "text = text.translate(remove_punc).lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tfidf\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fake entry to test\n",
    "features = tfidf.get_feature_names()\n",
    "response = tfidf.transform(['Malibu is a wonderful kitty cat who fell and was taken immediately LOVE! \\\n",
    "                             where a human without hair cared for it. Now love it needs a new home and you \\\n",
    "                             might be able to help. Please help. Call 0800-I-CARE now!!!!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken \t 0.354\n",
      "malibu \t 0.354\n",
      "love \t 0.707\n",
      "immedi \t 0.354\n",
      "human \t 0.354\n"
     ]
    }
   ],
   "source": [
    "# output non-zero\n",
    "for col in response.nonzero()[1]:\n",
    "    print(\"{} \\t {:.3f}\".format(features[col], response[0, col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of all test data descriptions\n",
    "token_dict = {}\n",
    "for idx, desc in df_test['Description'].items():\n",
    "    try:\n",
    "        token_dict[idx] = desc.translate(remove_punc).lower()\n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tfidf again\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10506"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check feature count - 10,506 (too many??)\n",
    "features = tfidf.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of all training data descriptions\n",
    "token_dict = {}\n",
    "for idx, desc in df_train['Description'].items():\n",
    "    try:\n",
    "        token_dict[idx] = desc.translate(remove_punc).lower()\n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tfidf for training data\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22103"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check training features count\n",
    "features_training = tfidf.get_feature_names()\n",
    "len(features_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6408"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersecting features between test and training\n",
    "combined = set(features) & set(features_training)\n",
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT: create dictionary of all test data descriptions\n",
    "token_dict = {}\n",
    "for idx, desc in df_test['Description'].items():\n",
    "    try:\n",
    "        token_dict[idx] = desc.translate(remove_punc).lower()\n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tfidf for test again\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine test and training data\n",
    "df_combined = pd.concat([df_test, df_train], sort=False)\n",
    "df_combined['test'] = df_combined['AdoptionSpeed'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10506"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check training features count\n",
    "features = tfidf.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description is not text/string object\n"
     ]
    }
   ],
   "source": [
    "# ValueError where missing description is represented as np.nan (not a number - float object)\n",
    "try:\n",
    "    response = tfidf.transform(df_combined['Description'])\n",
    "except ValueError:\n",
    "    print(\"Description is not text/string object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace np.nan with blank text\n",
    "df_combined['Description'] = df_combined['Description'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all data to test data tokens\n",
    "response = tfidf.transform(df_combined['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert response to array (if becomes issue will use sparse array but might need to convert rest of data)\n",
    "response_arr = response.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18941, 10506)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tested adding this to the df but got Memory error - will need to use sparse\n",
    "response_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "3076eefcc83e005977b9159c9d39459e5d08e27a"
   },
   "outputs": [],
   "source": [
    "def apply_word_flags(df, words):\n",
    "    \"\"\"Creates binary columns for words which appear in the description\"\"\"\n",
    "    for word in words:\n",
    "        df[word] = 0\n",
    "    for i, desc in df['Description'].items():\n",
    "        try:\n",
    "            for word in desc.split():\n",
    "                word = word.lower()\n",
    "                if word in words:\n",
    "                    df.at[i,word] = 1\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return df.drop(columns=['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['home', 'good' , 'adopt', 'loving', 'give', 'looking', 'playful', 'rescued', 'cat', 'contact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_color_flags(df, colors):\n",
    "    \"\"\"Combines Colors 1,2 & 3 into binary columns for each possible colours\"\"\"\n",
    "    for c in colors:\n",
    "        df[f'C{c}'] = 0\n",
    "    for i,colors in df[['Color1', 'Color2', 'Color3']].iterrows():\n",
    "        for c in colors:\n",
    "            if c != 0:\n",
    "                df.at[i,f'C{c}'] = 1\n",
    "    df = df.drop(columns=['Color1', 'Color2', 'Color3'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_breed_keywords(df):\n",
    "    \"\"\"Creates unique list of keywords from provided breeds dataframe\"\"\"\n",
    "    breed_keywords = []\n",
    "    for breed in df['BreedName']:\n",
    "        breed = re.sub(r'[/(/)]', '', breed)  # remove braces\n",
    "        keywords = breed.split()\n",
    "        breed_keywords += keywords\n",
    "    return set(breed_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_breed_flags(df, keywords, breeds):\n",
    "    \"\"\"Creates binary columns for keywords which appear in the breed name\"\"\"\n",
    "    for word in keywords:\n",
    "        df[word] = 0\n",
    "        \n",
    "    for i,pair in df[['Breed1', 'Breed2']].iterrows():\n",
    "        for indx in pair:\n",
    "            if indx == 0: continue\n",
    "            breed = breeds.loc[indx,'BreedName']\n",
    "            breed = re.sub(r'[/(/)]', '', breed)\n",
    "            new_keywords = breed.split()\n",
    "            for word in new_keywords:\n",
    "                if word in keywords: \n",
    "                    df.at[i,word] = 1\n",
    "                    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine test and training data\n",
    "df_combined = pd.concat([df_test, df_train], sort=False)\n",
    "df_combined['test'] = df_combined['AdoptionSpeed'].isna()\n",
    "\n",
    "# Rescuer\n",
    "rescue_map = Counter(df_combined['RescuerID'])\n",
    "rescuer_counts = df_combined['RescuerID'].map(rescue_map)\n",
    "\n",
    "# Breeds\n",
    "all_test_breeds = df_test['Breed1'].append(df_test['Breed2'])\n",
    "df_test_breeds = df_breeds.loc[all_test_breeds[all_test_breeds > 0].unique(), :]\n",
    "breed_keywords = create_breed_keywords(df_test_breeds)\n",
    "\n",
    "# Prepare data for modelling \n",
    "df_combined['rescuer_counts'] = rescuer_counts\n",
    "# df_combined = apply_word_flags(df_combined, keywords)\n",
    "df_combined = apply_color_flags(df_combined, colors)\n",
    "df_combined = apply_breed_flags(df_combined, breed_keywords, df_breeds)\n",
    "df_combined = pd.get_dummies(df_combined, columns=['Gender',\n",
    "                                                   'Vaccinated', 'Dewormed', 'Sterilized', \n",
    "                                                   'State'])\n",
    "y_train_all = df_combined['AdoptionSpeed'][df_combined['test'] != 1]\n",
    "X_all       = df_combined.drop(columns=['Name', 'RescuerID', 'AdoptionSpeed', 'Breed1', 'Breed2'])\n",
    "X_train_all = X_all[X_all['test'] != 1].drop(columns=['test'])\n",
    "X_test_all  = X_all[X_all['test'] == 1].drop(columns=['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with sparse data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  2, ...,  0,  0,  0],\n",
       "       [ 2, 24,  2, ...,  0,  0,  0],\n",
       "       [ 2, 20,  2, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 2,  2,  3, ...,  0,  0,  0],\n",
       "       [ 2,  9,  1, ...,  0,  0,  0],\n",
       "       [ 1,  1,  2, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert all data to numpy and convert to integers (scipy doesn't like mixed data types)\n",
    "X_all_values = np.array(X_all.values.astype(int))\n",
    "X_all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all data to sparse data array and remove test column\n",
    "X_all_sparse = scipy.sparse.csr_matrix(np.array(X_all.drop(columns=['test']).values.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame:(18941, 215)\n",
      "Sparse Array: \t (18941, 214)\n"
     ]
    }
   ],
   "source": [
    "# shape of sparse array, compared to pandas dataframe\n",
    "print(f\"Pandas DataFrame:{X_all.shape}\")\n",
    "print(f\"Sparse Array: \\t {X_all_sparse.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get index of test column\n",
    "list(X_all.columns).index('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all data into test and training\n",
    "mask_test = np.where(X_all['test'] == 1)[0]\n",
    "mask_train = np.where(X_all['test'] == 0)[0]\n",
    "X_train_sparse = X_all_sparse[mask_train]\n",
    "X_test_sparse  = X_all_sparse[mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# same as 'cv_testing' from below, but iloc removed and important features returned\n",
    "def cv_testing_sparse(X_train_all, params, folds=5):\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    folds = KFold(folds, True, rnd).split(X_train_all)\n",
    "\n",
    "    for train_indx, test_indx in folds:\n",
    "\n",
    "        X_train, X_test = X_train_all[train_indx], X_train_all[test_indx]\n",
    "        y_train, y_test = y_train_all[train_indx], y_train_all[test_indx]\n",
    "\n",
    "\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        prediction = clf.predict(X_test)\n",
    "\n",
    "        scores.append(ml_metrics.quadratic_weighted_kappa(rater_a=y_test, rater_b=prediction))\n",
    "        print(\"{:.3f}\".format(scores[-1]), end=\"\\t\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return scores, clf.feature_importances_ # latest fold features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:11:20] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.384\t[21:11:23] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.422\t[21:11:26] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.416\t[21:11:30] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.387\t[21:11:33] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.367\t[21:11:36] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.416\t[21:11:39] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.384\t[21:11:42] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.368\t[21:11:46] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.383\t[21:11:49] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.377\t\n"
     ]
    }
   ],
   "source": [
    "# scores are very much the same as using dataframe\n",
    "scores, _ = cv_testing_sparse(X_train_all=X_train_sparse, folds=10, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39048733629151666"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.390 (unchanged scores as expected)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all sparse data with description features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18941, 10506)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<18941x10506 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 499323 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check response \n",
    "print(response.shape)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18941, 214)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check sparse data\n",
    "X_all_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy hstack\n",
    "X_sparse_inc_desc = hstack([X_all_sparse, response]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18941, 10720)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<18941x10720 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 863884 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check new sparse dataset\n",
    "print(X_sparse_inc_desc.shape)\n",
    "X_sparse_inc_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all data into test and training\n",
    "mask_test = np.where(X_all['test'] == 1)[0]\n",
    "mask_train = np.where(X_all['test'] == 0)[0]\n",
    "X_train_sparse = X_sparse_inc_desc[mask_train]\n",
    "X_test_sparse  = X_sparse_inc_desc[mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:40:25] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.413\t[21:41:59] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.407\t[21:43:31] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.367\t[21:45:10] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.347\t[21:46:49] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.361\t[21:48:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.403\t[21:49:50] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.362\t[21:51:23] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.366\t[21:52:55] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.375\t[21:54:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.380\t\n"
     ]
    }
   ],
   "source": [
    "# scores are very much the same as using dataframe\n",
    "scores, feature_importances = cv_testing_sparse(X_train_all=X_train_sparse, folds=10, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00257175, 0.02880362, 0.00524637, ..., 0.        , 0.        ,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusted model to get feature importances, however needs labelling\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x01iq',\n",
       " '\\x01ǩbty9s̷\\x02˂0dgmoj\\x17qxbqu\\x05gd',\n",
       " '\\x03about',\n",
       " '\\x04\\x13ժ\\x7fdgjoqf\\x076',\n",
       " '\\x06b8tw\\x1bq眂̽ԣ\\x15nwg\\x7fnq\\x06iqgkz\\x0fl\\x15h\\x16gsŗm\\x05\\x04\\x0ecm\\x15f\\x1a\\x03esm\\x05y\\x05\\x1bs\\x05qttjywwh\\x16k\\x15ٯ\\x04',\n",
       " '\\x0fإw6k\\x17pvcxqxnt',\n",
       " '\\x10c\\x0fusjn9tlodnb\\x17w7ֹg\\x16\\x17g\\x7fr\\x15dl',\n",
       " '\\x13',\n",
       " '\\x14\\x17',\n",
       " '\\x14mh\\x10',\n",
       " '\\x175yz',\n",
       " '\\x18t5dl',\n",
       " '\\x1alys\\x12k',\n",
       " '\\x1avvh這mniysd\\x17ilu\\x18b\\x06\\x7f\\x01\\u07b9ab\\x12mvit9xgovē\\x11ẑlwet8r\\x1b\\x07zr',\n",
       " '0',\n",
       " '000',\n",
       " '02',\n",
       " '03',\n",
       " '06',\n",
       " '08',\n",
       " '09',\n",
       " '0for',\n",
       " '1',\n",
       " '10',\n",
       " '108',\n",
       " '109',\n",
       " '10am',\n",
       " '10in1',\n",
       " '10kg',\n",
       " '10lx3di',\n",
       " '10pm',\n",
       " '10second',\n",
       " '10th',\n",
       " '11',\n",
       " '1111',\n",
       " '113',\n",
       " '116',\n",
       " '11am',\n",
       " '11juli',\n",
       " '11month',\n",
       " '11pm',\n",
       " '11th',\n",
       " '12',\n",
       " '121',\n",
       " '1213',\n",
       " '122',\n",
       " '123',\n",
       " '12310',\n",
       " '128kg',\n",
       " '12916',\n",
       " '12hr',\n",
       " '12midnit',\n",
       " '12month',\n",
       " '12pm',\n",
       " '12pm2pm',\n",
       " '12th',\n",
       " '12yr',\n",
       " '13',\n",
       " '13615',\n",
       " '13814',\n",
       " '13th',\n",
       " '13yearsnow',\n",
       " '14',\n",
       " '142',\n",
       " '144',\n",
       " '145',\n",
       " '149',\n",
       " '14dogsreal',\n",
       " '14juli',\n",
       " '15',\n",
       " '156',\n",
       " '15715',\n",
       " '15jul',\n",
       " '15k',\n",
       " '15kg',\n",
       " '15minut',\n",
       " '15th',\n",
       " '16',\n",
       " '167',\n",
       " '168',\n",
       " '16jan',\n",
       " '16th',\n",
       " '17',\n",
       " '174',\n",
       " '175',\n",
       " '1751',\n",
       " '17july15',\n",
       " '17kg',\n",
       " '17th',\n",
       " '18',\n",
       " '184',\n",
       " '18kg',\n",
       " '18th',\n",
       " '19',\n",
       " '197',\n",
       " '198',\n",
       " '19913',\n",
       " '19th',\n",
       " '1am',\n",
       " '1ami',\n",
       " '1and',\n",
       " '1black',\n",
       " '1c',\n",
       " '1femal',\n",
       " '1free',\n",
       " '1give',\n",
       " '1have',\n",
       " '1i',\n",
       " '1keep',\n",
       " '1male4femal',\n",
       " '1month',\n",
       " '1mth',\n",
       " '1must',\n",
       " '1oprah',\n",
       " '1sassi',\n",
       " '1sometim',\n",
       " '1spring',\n",
       " '1st',\n",
       " '1sunshin',\n",
       " '1suzi',\n",
       " '1whitey',\n",
       " '1y',\n",
       " '1year',\n",
       " '1your',\n",
       " '1yr',\n",
       " '1一定要养在室内。',\n",
       " '1必须带它结扎和打预防针',\n",
       " '1）在你买我之前你需要知道，我这一生大概只能活年，和你分别是件无比痛苦的事。',\n",
       " '1）在你买我之前你需要知道，我这一生大概只能活年，和你分别是件无比痛苦的事。mi',\n",
       " '2',\n",
       " '20',\n",
       " '202',\n",
       " '209',\n",
       " '20th',\n",
       " '21',\n",
       " '210',\n",
       " '213',\n",
       " '218',\n",
       " '21st',\n",
       " '21th',\n",
       " '22213',\n",
       " '223',\n",
       " '22713',\n",
       " '228',\n",
       " '22k',\n",
       " '22nd',\n",
       " '23',\n",
       " '234',\n",
       " '23808',\n",
       " '23kgfatti',\n",
       " '23month',\n",
       " '23rd',\n",
       " '23个月大',\n",
       " '24',\n",
       " '242',\n",
       " '24615',\n",
       " '247',\n",
       " '248',\n",
       " '24813',\n",
       " '24hr',\n",
       " '24th',\n",
       " '25',\n",
       " '25juli',\n",
       " '25kg',\n",
       " '25th',\n",
       " '26',\n",
       " '263',\n",
       " '26811',\n",
       " '26kg',\n",
       " '26th',\n",
       " '26thbut',\n",
       " '27',\n",
       " '27318',\n",
       " '279',\n",
       " '27th',\n",
       " '28',\n",
       " '281',\n",
       " '28814',\n",
       " '28th',\n",
       " '29',\n",
       " '29912',\n",
       " '29c',\n",
       " '29th',\n",
       " '2agre',\n",
       " '2allow',\n",
       " '2brother',\n",
       " '2cat',\n",
       " '2day',\n",
       " '2dog',\n",
       " '2femal',\n",
       " '2have',\n",
       " '2if',\n",
       " '2izzi',\n",
       " '2k3k',\n",
       " '2kg',\n",
       " '2kirbi',\n",
       " '2live',\n",
       " '2mi',\n",
       " '2molo',\n",
       " '2month',\n",
       " '2monthold',\n",
       " '2mrw',\n",
       " '2mth',\n",
       " '2nd',\n",
       " '2pup',\n",
       " '2sherbet',\n",
       " '2side',\n",
       " '2sistar',\n",
       " '2summer',\n",
       " '2vaccin',\n",
       " '2week',\n",
       " '2weeksold',\n",
       " '2wish',\n",
       " '2x',\n",
       " '2x\\x0f7\\x03\\x15\\x07mz9x\\x0e\\x1b\\x11ơ',\n",
       " '2year',\n",
       " '2yr',\n",
       " '2°',\n",
       " '2必须养在家里',\n",
       " '2必须要打预防针和结扎。',\n",
       " '2隻都好兇，我家度咕被霸凌了',\n",
       " '2）在给我命令时请给我理解的时间，别对我发脾气，虽然我一定会原谅你的，你的耐心和理解能让我学得更快。',\n",
       " '2）在给我命令时请给我理解的时间，别对我发脾气，虽然我一定会原谅你的，你的耐心和理解能让我学得更快。give',\n",
       " '3',\n",
       " '30',\n",
       " '30kg',\n",
       " '30l',\n",
       " '30rm',\n",
       " '30th',\n",
       " '31',\n",
       " '318',\n",
       " '31st',\n",
       " '31th',\n",
       " '32',\n",
       " '32beg',\n",
       " '34',\n",
       " '34mnth',\n",
       " '35',\n",
       " '354',\n",
       " '36',\n",
       " '36they',\n",
       " '37kg',\n",
       " '38',\n",
       " '38kg',\n",
       " '3am',\n",
       " '3autumn',\n",
       " '3can',\n",
       " '3cant',\n",
       " '3cat',\n",
       " '3day',\n",
       " '3have',\n",
       " '3keep',\n",
       " '3kg',\n",
       " '3kitten',\n",
       " '3lane',\n",
       " '3leg',\n",
       " '3male',\n",
       " '3month',\n",
       " '3monthold',\n",
       " '3rd',\n",
       " '3reason',\n",
       " '3storey',\n",
       " '3they',\n",
       " '3week',\n",
       " '3wish',\n",
       " '3x',\n",
       " '3year',\n",
       " '3yo',\n",
       " '3yr',\n",
       " '3领养后一定要更新毛孩的生活照让我知道毛孩的近况。',\n",
       " '3领养后必须提供它的生活照让我知道它还安好',\n",
       " '3）请好好对我，因为世界上最珍惜最需要你的爱心的是我，别生气太久，也别把我关起来，因为，你有你的生活，你的朋友，你的工作和娱乐，而我，只',\n",
       " '4',\n",
       " '40cm',\n",
       " '41',\n",
       " '410',\n",
       " '43',\n",
       " '45',\n",
       " '45week',\n",
       " '46',\n",
       " '49',\n",
       " '4aug',\n",
       " '4barf',\n",
       " '4bulan',\n",
       " '4can',\n",
       " '4cat',\n",
       " '4cute',\n",
       " '4day',\n",
       " '4dog',\n",
       " '4ft',\n",
       " '4kg',\n",
       " '4kitten',\n",
       " '4month',\n",
       " '4mth',\n",
       " '4pm',\n",
       " '4rm',\n",
       " '4show',\n",
       " '4th',\n",
       " '4thaugust',\n",
       " '4u',\n",
       " '4week',\n",
       " '4winter',\n",
       " '4x4',\n",
       " '4希望领养者能捐赠猫砂或猫饼，协助我救助更多的流浪猫咪',\n",
       " '4我会向您收取结扎定金',\n",
       " '4我会向您收取领养费',\n",
       " '4）经常和我说话吧，虽然我听不懂你的语言，但我认得你的声音，你是知道的，在你回家时我是多么高兴，因为我一直在竖着耳朵等待你的脚步声',\n",
       " '5',\n",
       " '5\\x13ѡ֭\\x10ïj',\n",
       " '5\\x1bsmn69zv\\x19',\n",
       " '50',\n",
       " '505kg',\n",
       " '510',\n",
       " '512',\n",
       " '51kg',\n",
       " '530pm',\n",
       " '55',\n",
       " '558',\n",
       " '55kg',\n",
       " '56',\n",
       " '56个月大的时候带去结扎，结扎后我会把',\n",
       " '57',\n",
       " '57day',\n",
       " '58',\n",
       " '59',\n",
       " '5cm',\n",
       " '5in1',\n",
       " '5keep',\n",
       " '5kg',\n",
       " '5male',\n",
       " '5mnth',\n",
       " '5month',\n",
       " '5mth',\n",
       " '5pm',\n",
       " '5th',\n",
       " '5th6th',\n",
       " '5treat',\n",
       " '5week',\n",
       " '5will',\n",
       " '5yo',\n",
       " '5）请注意你对待我的好，我永远不会忘记它，如果它是残酷的，可能会影响我永远',\n",
       " '6',\n",
       " '601',\n",
       " '610',\n",
       " '63',\n",
       " '67',\n",
       " '67month',\n",
       " '68',\n",
       " '68month',\n",
       " '69kg',\n",
       " '6eat',\n",
       " '6kg',\n",
       " '6litter',\n",
       " '6month',\n",
       " '6monthsi',\n",
       " '6mth',\n",
       " '6pm',\n",
       " '6th',\n",
       " '6vn\\x10ba83\\x1bco\\x08\\x1agqqqjdtzrybr\\x1aib',\n",
       " '6week',\n",
       " '6）请',\n",
       " '7',\n",
       " '710month',\n",
       " '711',\n",
       " '72',\n",
       " '72a',\n",
       " '730pm',\n",
       " '75kg',\n",
       " '7613',\n",
       " '78',\n",
       " '7day',\n",
       " '7dog',\n",
       " '7eleven',\n",
       " '7month',\n",
       " '7monthold',\n",
       " '7owner',\n",
       " '7puppi',\n",
       " '7th',\n",
       " '7week',\n",
       " '7）在你觉得我懒，不再又跑又跳或者不听话时，在骂我之前，请想想也许我出了什么问题，也许我吃的东西不对，也许我病了，也许我已经老了。',\n",
       " '7）在你觉得我懒，不再又跑又跳或者不听话时，在骂我之前，请想想也许我出了什么问题，也许我吃的东西不对，也许我病了，也许我已经老了。befor',\n",
       " '8',\n",
       " '80',\n",
       " '8009',\n",
       " '81',\n",
       " '810month',\n",
       " '830pm',\n",
       " '84',\n",
       " '84jalan',\n",
       " '84kg',\n",
       " '8615',\n",
       " '88',\n",
       " '8kg',\n",
       " '8month',\n",
       " '8pm10pm',\n",
       " '8th',\n",
       " '8week',\n",
       " '8yrold',\n",
       " '8）当我老了，不再像小宝贝时那么可爱时，请你仍然对我好，仍然',\n",
       " '9',\n",
       " '90',\n",
       " '90⋤\\x05eu',\n",
       " '9515',\n",
       " '97k\\x0ed\\x13rmd',\n",
       " '9am5pm',\n",
       " '9pm',\n",
       " '9th',\n",
       " '9week',\n",
       " '9wыl\\x19\\x15y\\x04\\x0ekӵ\\x0f',\n",
       " '9）当我已经很老的时候，当我的健康已经逝去，已无法正常的生活，请不要想方设法让我继续活下去，因为我已经不行了，我知道你也不想我离开，但请接受这个事实，并在最后的时刻与我在一起，求求你一定不要说我不忍心看它死去而走开，因为在我生命的最后一刻，如果能在你怀中离开这个世界，听着你的声音，我就什么都不怕，你就是我的家，我爱你！',\n",
       " '9）当我已经很老的时候，当我的健康已经逝去，已无法正常的生活，请不要想方设法让我继续活下去，因为我已经不行了，我知道你也不想我离开，但请接受这个事实，并在最后的时刻与我在一起，求求你一定不要说我不忍心看它死去而走开，因为在我生命的最后一刻，如果能在你怀中离开这个世界，听着你的声音，我就什么都不怕，你就是我的家，我爱你！go',\n",
       " 'a1',\n",
       " 'a2',\n",
       " 'a3',\n",
       " 'aaawwww',\n",
       " 'aaliyah',\n",
       " 'aang',\n",
       " 'aapa',\n",
       " 'abadon',\n",
       " 'abandon',\n",
       " 'abandoneddump',\n",
       " 'abang',\n",
       " 'abangjntn',\n",
       " 'abbi',\n",
       " 'abdomen',\n",
       " 'abeauti',\n",
       " 'abg',\n",
       " 'abid',\n",
       " 'abil',\n",
       " 'abit',\n",
       " 'abl',\n",
       " 'abnorm',\n",
       " 'aboard',\n",
       " 'abod',\n",
       " 'abondon',\n",
       " 'aboo',\n",
       " 'abort',\n",
       " 'abov',\n",
       " 'aboymal',\n",
       " 'abras',\n",
       " 'abroad',\n",
       " 'abscess',\n",
       " 'absenc',\n",
       " 'absent',\n",
       " 'absolut',\n",
       " 'absoulut',\n",
       " 'abt',\n",
       " 'abu',\n",
       " 'abund',\n",
       " 'abus',\n",
       " 'abusiveso',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessori',\n",
       " 'accessoriesfood',\n",
       " 'accid',\n",
       " 'accident',\n",
       " 'accomad',\n",
       " 'accommod',\n",
       " 'accomod',\n",
       " 'accompani',\n",
       " 'accord',\n",
       " 'accordingli',\n",
       " 'account',\n",
       " 'accountethanol',\n",
       " 'accumul',\n",
       " 'accustom',\n",
       " 'acd',\n",
       " 'ace',\n",
       " 'ach',\n",
       " 'achiev',\n",
       " 'acknowledg',\n",
       " 'acquaint',\n",
       " 'acquir',\n",
       " 'acrobat',\n",
       " 'acsyen',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'activeand',\n",
       " 'activeboth',\n",
       " 'activeeat',\n",
       " 'activehav',\n",
       " 'activehealthi',\n",
       " 'activemak',\n",
       " 'activemom',\n",
       " 'activeplayfulpamp',\n",
       " 'activer',\n",
       " 'activetoilet',\n",
       " 'active，playful，',\n",
       " 'activit',\n",
       " 'actual',\n",
       " 'acumen',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adakah',\n",
       " 'adalah',\n",
       " 'adalh',\n",
       " 'adam',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'adder',\n",
       " 'addict',\n",
       " 'addit',\n",
       " 'address',\n",
       " 'ade',\n",
       " 'adeq',\n",
       " 'adequ',\n",
       " 'ader',\n",
       " 'adher',\n",
       " 'adi',\n",
       " 'adik',\n",
       " 'adikberadik',\n",
       " 'adikbetina',\n",
       " 'adiknya',\n",
       " 'adjust',\n",
       " 'admin',\n",
       " 'administ',\n",
       " 'administr',\n",
       " 'admit',\n",
       " 'adobt',\n",
       " 'adolesc',\n",
       " 'adopt',\n",
       " 'adoptdontshop',\n",
       " 'adopte',\n",
       " 'adoptedi',\n",
       " 'adopterpet',\n",
       " 'adopterscontact',\n",
       " 'adoptersfoster',\n",
       " 'adopterswho',\n",
       " 'adoptfamili',\n",
       " 'adopthomeless',\n",
       " 'adoptian',\n",
       " 'adoptingview',\n",
       " 'adoptioan',\n",
       " 'adoption2',\n",
       " 'adoptionand',\n",
       " 'adoptionfost',\n",
       " 'adoptionh',\n",
       " 'adoptionhealthi',\n",
       " 'adoptionkt',\n",
       " 'adoptionnow',\n",
       " 'adoptionpick',\n",
       " 'adoptionpleas',\n",
       " 'adoptionveri',\n",
       " 'adoption，',\n",
       " 'adoption：',\n",
       " 'adoptjantanaktifmanja',\n",
       " 'adoptjust',\n",
       " 'adoptor',\n",
       " 'adoptthank',\n",
       " 'adopt。',\n",
       " 'ador',\n",
       " 'adorableeasi',\n",
       " 'adorableextrem',\n",
       " 'adorablefost',\n",
       " 'adorablenessess',\n",
       " 'adorableplay',\n",
       " 'adorableplayfullcut',\n",
       " 'adorabletoilet',\n",
       " 'adpot',\n",
       " 'adressno7jalan',\n",
       " 'adrien',\n",
       " 'adult',\n",
       " 'advanc',\n",
       " 'advantag',\n",
       " 'advantur',\n",
       " 'adventouroush',\n",
       " 'adventur',\n",
       " 'adversari',\n",
       " 'advertis',\n",
       " 'advic',\n",
       " 'advis',\n",
       " 'advoc',\n",
       " 'affect',\n",
       " 'affection',\n",
       " 'affectionatepleas',\n",
       " 'affogato',\n",
       " 'afford',\n",
       " 'affordboth',\n",
       " 'affort',\n",
       " 'afraid',\n",
       " 'afriad',\n",
       " 'afteral',\n",
       " 'afternoon',\n",
       " 'afterward',\n",
       " 'agak',\n",
       " 'age',\n",
       " 'age2month',\n",
       " 'age3',\n",
       " 'aggres',\n",
       " 'aggress',\n",
       " 'agil',\n",
       " 'ago',\n",
       " 'agoabout',\n",
       " 'agoit',\n",
       " 'agoni',\n",
       " 'agoour',\n",
       " 'agosh',\n",
       " 'agotoday',\n",
       " 'agowa',\n",
       " 'agre',\n",
       " 'agreeabl',\n",
       " 'agreedso',\n",
       " 'agreement',\n",
       " 'agresif',\n",
       " 'agress',\n",
       " 'agricultur',\n",
       " 'agriland',\n",
       " 'ah',\n",
       " 'ahad',\n",
       " 'ahahah',\n",
       " 'ahe',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhaaath',\n",
       " 'ai',\n",
       " 'aida',\n",
       " 'aika',\n",
       " 'aila',\n",
       " 'aileen',\n",
       " 'aim',\n",
       " 'aime',\n",
       " 'aimi',\n",
       " 'aimlessli',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'aira',\n",
       " 'aircon',\n",
       " 'airport',\n",
       " 'aisyah',\n",
       " 'ajar',\n",
       " 'aka',\n",
       " 'akak',\n",
       " 'akan',\n",
       " 'akila',\n",
       " 'akio',\n",
       " 'akira',\n",
       " 'akita',\n",
       " 'akk',\n",
       " 'akleh',\n",
       " 'akn',\n",
       " 'aktif',\n",
       " 'aktifsuk',\n",
       " 'aktifsuka',\n",
       " 'aktiv',\n",
       " 'akurung',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'ala2',\n",
       " 'aladdin',\n",
       " 'alam',\n",
       " 'alam1',\n",
       " 'alamklklangsubang',\n",
       " 'alan',\n",
       " 'alang',\n",
       " 'alani',\n",
       " 'alarm',\n",
       " 'alarmist',\n",
       " 'alasanpoo',\n",
       " 'alaska',\n",
       " 'albeit',\n",
       " 'album',\n",
       " 'ald',\n",
       " 'aldou',\n",
       " 'alerg',\n",
       " 'alergi',\n",
       " 'alert',\n",
       " 'alertplayfulonli',\n",
       " 'aleryplayful',\n",
       " 'alex',\n",
       " 'alexand',\n",
       " 'alexi',\n",
       " 'alfa',\n",
       " 'alhamdulillah',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'alik',\n",
       " 'alin',\n",
       " 'alireza',\n",
       " 'aliv',\n",
       " 'aliya',\n",
       " 'allaerg',\n",
       " 'allah',\n",
       " 'allerg',\n",
       " 'allergen',\n",
       " 'allergi',\n",
       " 'alley',\n",
       " 'alli',\n",
       " 'alloc',\n",
       " 'allow',\n",
       " 'allsaya',\n",
       " 'allyssa',\n",
       " 'aload',\n",
       " 'alon',\n",
       " 'aloneangel',\n",
       " 'alonesh',\n",
       " 'alongif',\n",
       " 'aloof',\n",
       " 'alor',\n",
       " 'alot',\n",
       " 'alotwelltrainedfriendli',\n",
       " 'alpha',\n",
       " 'alr',\n",
       " 'alreadi',\n",
       " 'alreadycurr',\n",
       " 'alreadyhiii',\n",
       " 'alreadyim',\n",
       " 'alreadyw',\n",
       " 'already，',\n",
       " 'alreday',\n",
       " 'alright',\n",
       " 'alsatian',\n",
       " 'altern',\n",
       " 'althlet',\n",
       " 'altogeth',\n",
       " 'alvin',\n",
       " 'alway',\n",
       " 'alwez',\n",
       " 'alwi',\n",
       " 'aman',\n",
       " 'amanda',\n",
       " 'amat',\n",
       " 'amatlah',\n",
       " 'amaz',\n",
       " 'amazingli',\n",
       " 'ambank',\n",
       " 'amber',\n",
       " 'ambik',\n",
       " 'ambil',\n",
       " 'ambilkan',\n",
       " 'ambush',\n",
       " 'amc',\n",
       " 'ameri',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amik',\n",
       " 'aminahth',\n",
       " 'amiss',\n",
       " 'amma',\n",
       " 'ampang',\n",
       " 'ampanga',\n",
       " 'ampangdevelop',\n",
       " 'ampl',\n",
       " 'ampleas',\n",
       " 'amput',\n",
       " 'amur',\n",
       " 'amus',\n",
       " 'anabella',\n",
       " 'anak',\n",
       " 'anak2',\n",
       " 'anak2nya',\n",
       " 'anakanak',\n",
       " 'anakni',\n",
       " 'anakny',\n",
       " 'ancak',\n",
       " 'ancestri',\n",
       " 'anchovi',\n",
       " 'ancient',\n",
       " 'anda',\n",
       " 'andalasklangselangor',\n",
       " 'andhaniza',\n",
       " 'andhav',\n",
       " 'andi',\n",
       " 'andor',\n",
       " 'anem',\n",
       " 'anemia',\n",
       " 'anf',\n",
       " 'ang',\n",
       " 'angah',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angi',\n",
       " 'angkat',\n",
       " 'angl',\n",
       " 'angora',\n",
       " 'angri',\n",
       " 'angu',\n",
       " 'ani',\n",
       " 'anim',\n",
       " 'animalcar',\n",
       " 'animald',\n",
       " 'animallov',\n",
       " 'anita',\n",
       " 'anj',\n",
       " 'ank',\n",
       " 'ank2',\n",
       " 'ankl',\n",
       " 'anknya',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anni',\n",
       " 'announc',\n",
       " 'annoy',\n",
       " 'annual',\n",
       " 'anonym',\n",
       " 'anoth',\n",
       " 'anotheri',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'antenna',\n",
       " 'anti',\n",
       " 'antibiot',\n",
       " 'antic',\n",
       " 'anticip',\n",
       " 'antidog',\n",
       " 'antifung',\n",
       " 'antifungu',\n",
       " 'antitick',\n",
       " 'anubi',\n",
       " 'anujah',\n",
       " 'anxieti',\n",
       " 'anxiou',\n",
       " 'any1',\n",
       " 'anya',\n",
       " 'anybodi',\n",
       " 'anybodyto',\n",
       " 'anymor',\n",
       " 'anymorei',\n",
       " 'anymorepl',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'anythinh',\n",
       " 'anytim',\n",
       " 'anywher',\n",
       " 'aoki',\n",
       " 'aound',\n",
       " 'apa',\n",
       " 'apa2',\n",
       " 'apaapa',\n",
       " 'apabila',\n",
       " 'apam',\n",
       " 'apart',\n",
       " 'apartmentnot',\n",
       " 'apartmentpleas',\n",
       " 'apartmentw',\n",
       " 'apartmentwith',\n",
       " 'ape',\n",
       " 'apertur',\n",
       " 'apetit',\n",
       " 'api',\n",
       " 'apolog',\n",
       " 'apologet',\n",
       " 'aporeci',\n",
       " 'app',\n",
       " 'appal',\n",
       " 'appar',\n",
       " 'appartmen',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appetit',\n",
       " 'appl',\n",
       " 'appleth',\n",
       " 'appli',\n",
       " 'applic',\n",
       " 'appoint',\n",
       " 'appologis',\n",
       " 'appreaci',\n",
       " 'appreci',\n",
       " 'apprehens',\n",
       " 'approach',\n",
       " 'approch',\n",
       " 'appropri',\n",
       " 'approri',\n",
       " 'approv',\n",
       " 'approx',\n",
       " 'approxim',\n",
       " 'apr',\n",
       " 'apricot',\n",
       " 'april',\n",
       " 'aprod',\n",
       " 'aprox',\n",
       " 'aprtment',\n",
       " 'apt',\n",
       " 'aqap',\n",
       " 'ara',\n",
       " 'arang',\n",
       " 'arap',\n",
       " 'archi',\n",
       " 'ard',\n",
       " 'area',\n",
       " 'areaconcern',\n",
       " 'areacurr',\n",
       " 'areah',\n",
       " 'areajika',\n",
       " 'areanearbi',\n",
       " 'areapl',\n",
       " 'areatikki',\n",
       " 'areaw',\n",
       " 'area－',\n",
       " 'arent',\n",
       " 'argu',\n",
       " 'ari',\n",
       " 'ariel',\n",
       " 'arm',\n",
       " 'armpit',\n",
       " 'aroma',\n",
       " 'arounda',\n",
       " 'aroundand',\n",
       " 'aroundbecaus',\n",
       " 'aroundhealthi',\n",
       " 'aroundi',\n",
       " 'aroundlazypleas',\n",
       " 'aroundsh',\n",
       " 'aroundtheclock',\n",
       " 'aroundveri',\n",
       " 'aroundw',\n",
       " 'arrang',\n",
       " 'arrh',\n",
       " 'arriv',\n",
       " 'artist',\n",
       " 'arwah',\n",
       " 'arya',\n",
       " 'asalkan',\n",
       " 'asalnya',\n",
       " 'asap',\n",
       " 'asapa',\n",
       " 'asapfriendli',\n",
       " 'asapsadlyi',\n",
       " 'asapshel',\n",
       " 'ase',\n",
       " 'ash',\n",
       " 'ashcolour',\n",
       " 'ashley',\n",
       " 'ashleymeow88',\n",
       " 'ashton',\n",
       " 'asia',\n",
       " 'asid',\n",
       " 'asingkan',\n",
       " 'ask',\n",
       " 'asleep',\n",
       " 'asleeplov',\n",
       " 'asma',\n",
       " 'assalam',\n",
       " 'assalamualaikum',\n",
       " 'assault',\n",
       " 'assess',\n",
       " 'asset',\n",
       " 'assign',\n",
       " 'assimil',\n",
       " 'assist',\n",
       " 'associ',\n",
       " 'assum',\n",
       " 'assur',\n",
       " 'aster',\n",
       " 'asthma',\n",
       " 'asthmat',\n",
       " 'aswel',\n",
       " 'asyik',\n",
       " 'ata',\n",
       " 'atasan',\n",
       " 'atasi',\n",
       " 'atau',\n",
       " 'atchoi',\n",
       " 'ate',\n",
       " 'athlet',\n",
       " 'athma',\n",
       " 'ati',\n",
       " 'atleast',\n",
       " 'atlin',\n",
       " 'atonli',\n",
       " 'atop',\n",
       " 'atpl',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attent',\n",
       " 'attentionif',\n",
       " 'attentioninclud',\n",
       " 'attentionpleas',\n",
       " 'attentionso',\n",
       " 'atthank',\n",
       " 'attic',\n",
       " 'attitud',\n",
       " 'attn',\n",
       " 'attract',\n",
       " 'atw',\n",
       " 'au3',\n",
       " 'audienc',\n",
       " 'auditor',\n",
       " 'audrey',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'augustseptemb',\n",
       " 'aunt',\n",
       " 'aunti',\n",
       " 'aurora',\n",
       " 'aussi',\n",
       " 'australia',\n",
       " 'author',\n",
       " 'automat',\n",
       " 'autralia',\n",
       " 'autumn',\n",
       " 'avail',\n",
       " 'avatar',\n",
       " 'avenu',\n",
       " 'averag',\n",
       " 'averi',\n",
       " ...]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10720"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine labels from original features and tfidf features\n",
    "labels = list(X_all.drop(columns=['test']).columns) + features\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe combining labels with importance and ordering in ascending order\n",
    "importance_df = pd.DataFrame(feature_importances,\n",
    "                             index = labels,\n",
    "                             columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rescuer_counts</th>\n",
       "      <td>0.031684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.028804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>0.023763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adopt</th>\n",
       "      <td>0.020574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veri</th>\n",
       "      <td>0.016048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.014299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>0.013887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.010493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleas</th>\n",
       "      <td>0.010390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wa</th>\n",
       "      <td>0.008950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play</th>\n",
       "      <td>0.008744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitten</th>\n",
       "      <td>0.008024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ha</th>\n",
       "      <td>0.007818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.007818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>0.007715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed</th>\n",
       "      <td>0.007715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thi</th>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puppi</th>\n",
       "      <td>0.007098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.006995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>care</th>\n",
       "      <td>0.006892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owner</th>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthi</th>\n",
       "      <td>0.006481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>0.006378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onli</th>\n",
       "      <td>0.005966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>0.005555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaturitySize</th>\n",
       "      <td>0.005246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.005144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>0.005144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>0.004938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gina</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gingerbread</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gentli</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gingerorang</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gingiv</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gingviti</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girland</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girlblu</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girlcat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girlestim</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gift</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gibsoni</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gib</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>giant</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genuin</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geok</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>georg</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>georgetown</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geram</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germani</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gestur</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getsthink</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gf</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gg</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gg2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gheko</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ghost</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ghoulish</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>🙀</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10720 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance\n",
       "rescuer_counts    0.031684\n",
       "Age               0.028804\n",
       "PhotoAmt          0.023763\n",
       "adopt             0.020574\n",
       "veri              0.016048\n",
       "love              0.014299\n",
       "home              0.013887\n",
       "dog               0.010493\n",
       "pleas             0.010390\n",
       "wa                0.008950\n",
       "play              0.008744\n",
       "kitten            0.008024\n",
       "ha                0.007818\n",
       "cat               0.007818\n",
       "Quantity          0.007715\n",
       "Mixed             0.007715\n",
       "thi               0.007407\n",
       "puppi             0.007098\n",
       "good              0.006995\n",
       "care              0.006892\n",
       "look              0.006584\n",
       "owner             0.006584\n",
       "healthi           0.006481\n",
       "Fee               0.006378\n",
       "onli              0.005966\n",
       "FurLength         0.005555\n",
       "MaturitySize      0.005246\n",
       "month             0.005144\n",
       "contact           0.005144\n",
       "C2                0.004938\n",
       "...                    ...\n",
       "gina              0.000000\n",
       "gingerbread       0.000000\n",
       "gentli            0.000000\n",
       "gingerorang       0.000000\n",
       "gingiv            0.000000\n",
       "gingviti          0.000000\n",
       "girland           0.000000\n",
       "girlblu           0.000000\n",
       "girlcat           0.000000\n",
       "girlestim         0.000000\n",
       "gift              0.000000\n",
       "gibsoni           0.000000\n",
       "gib               0.000000\n",
       "giant             0.000000\n",
       "genuin            0.000000\n",
       "geok              0.000000\n",
       "georg             0.000000\n",
       "georgetown        0.000000\n",
       "geram             0.000000\n",
       "germani           0.000000\n",
       "gestur            0.000000\n",
       "getsthink         0.000000\n",
       "gf                0.000000\n",
       "gg                0.000000\n",
       "gg2               0.000000\n",
       "gheko             0.000000\n",
       "ghost             0.000000\n",
       "ghoulish          0.000000\n",
       "gi                0.000000\n",
       "🙀                 0.000000\n",
       "\n",
       "[10720 rows x 1 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rescuer_counts</th>\n",
       "      <td>0.031684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.028804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>0.023763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adopt</th>\n",
       "      <td>0.020574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veri</th>\n",
       "      <td>0.016048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.014299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>0.013887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.010493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleas</th>\n",
       "      <td>0.010390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wa</th>\n",
       "      <td>0.008950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play</th>\n",
       "      <td>0.008744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitten</th>\n",
       "      <td>0.008024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ha</th>\n",
       "      <td>0.007818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.007818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>0.007715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed</th>\n",
       "      <td>0.007715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thi</th>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puppi</th>\n",
       "      <td>0.007098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.006995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>care</th>\n",
       "      <td>0.006892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owner</th>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthi</th>\n",
       "      <td>0.006481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>0.006378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onli</th>\n",
       "      <td>0.005966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>0.005555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaturitySize</th>\n",
       "      <td>0.005246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.005144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>0.005144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>0.004938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mka</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouth</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usual</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpsp</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appli</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aunti</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kota</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afternoon</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veterinari</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attitud</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mojo</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morn</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ur</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whatev</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alot</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alor</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibrant</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ambil</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attend</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurap</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moo</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appetit</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appear</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitamin</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anak2</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance\n",
       "rescuer_counts    0.031684\n",
       "Age               0.028804\n",
       "PhotoAmt          0.023763\n",
       "adopt             0.020574\n",
       "veri              0.016048\n",
       "love              0.014299\n",
       "home              0.013887\n",
       "dog               0.010493\n",
       "pleas             0.010390\n",
       "wa                0.008950\n",
       "play              0.008744\n",
       "kitten            0.008024\n",
       "ha                0.007818\n",
       "cat               0.007818\n",
       "Quantity          0.007715\n",
       "Mixed             0.007715\n",
       "thi               0.007407\n",
       "puppi             0.007098\n",
       "good              0.006995\n",
       "care              0.006892\n",
       "look              0.006584\n",
       "owner             0.006584\n",
       "healthi           0.006481\n",
       "Fee               0.006378\n",
       "onli              0.005966\n",
       "FurLength         0.005555\n",
       "MaturitySize      0.005246\n",
       "month             0.005144\n",
       "contact           0.005144\n",
       "C2                0.004938\n",
       "...                    ...\n",
       "money             0.000103\n",
       "mka               0.000103\n",
       "mouth             0.000103\n",
       "usual             0.000103\n",
       "mpsp              0.000103\n",
       "appli             0.000103\n",
       "aunti             0.000103\n",
       "kota              0.000103\n",
       "afternoon         0.000103\n",
       "veterinari        0.000103\n",
       "attitud           0.000103\n",
       "mojo              0.000103\n",
       "morn              0.000103\n",
       "ur                0.000103\n",
       "la                0.000103\n",
       "whatev            0.000103\n",
       "understand        0.000103\n",
       "alot              0.000103\n",
       "alor              0.000103\n",
       "vibrant           0.000103\n",
       "ambil             0.000103\n",
       "attend            0.000103\n",
       "mr                0.000103\n",
       "kurap             0.000103\n",
       "13                0.000103\n",
       "moo               0.000103\n",
       "appetit           0.000103\n",
       "appear            0.000103\n",
       "vitamin           0.000103\n",
       "anak2             0.000103\n",
       "\n",
       "[1279 rows x 1 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only positive importance makes up 1279, which will significantly improve speed at least\n",
    "nonzero_importance = importance_df[importance_df['importance'] > 0]\n",
    "nonzero_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun classifier for ALL training data and extract feature importances from run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:56:33] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.2, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=3, nthread=None, objective='multi:softprob', random_state=42,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8, tree_method='hist', verbose=0)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create classifier to train all data, to pick out important features\n",
    "clf = xgb.XGBClassifier(**params)\n",
    "clf.fit(X_train_sparse, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract important features\n",
    "importance_df = pd.DataFrame(clf.feature_importances_,\n",
    "                             index = labels,\n",
    "                             columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.027684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescuer_counts</th>\n",
       "      <td>0.027479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>0.024813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adopt</th>\n",
       "      <td>0.019994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>0.016098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance\n",
       "Age               0.027684\n",
       "rescuer_counts    0.027479\n",
       "PhotoAmt          0.024813\n",
       "adopt             0.019994\n",
       "home              0.016098"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.027684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescuer_counts</th>\n",
       "      <td>0.027479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>0.024813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adopt</th>\n",
       "      <td>0.019994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>0.016098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.013534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veri</th>\n",
       "      <td>0.012714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleas</th>\n",
       "      <td>0.011996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.010971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play</th>\n",
       "      <td>0.009125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wa</th>\n",
       "      <td>0.008203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>0.007895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.007587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed</th>\n",
       "      <td>0.007485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitten</th>\n",
       "      <td>0.007485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owner</th>\n",
       "      <td>0.007382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.007280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ha</th>\n",
       "      <td>0.007177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>care</th>\n",
       "      <td>0.006972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>0.006870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>0.006357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thi</th>\n",
       "      <td>0.006357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>0.006357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>0.006254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onli</th>\n",
       "      <td>0.005332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>0.005332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaturitySize</th>\n",
       "      <td>0.005229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>0.005229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activ</th>\n",
       "      <td>0.004922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.004716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mereka</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anak2</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advic</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whatsap</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whatapp</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needi</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjust</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>molli</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>note</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mamak</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandarin</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marbl</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jojo</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>untuk</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ye</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lap</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mana</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mojo</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>😊</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atau</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lain</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unless</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>athlet</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noth</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nose</th>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1305 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance\n",
       "Age               0.027684\n",
       "rescuer_counts    0.027479\n",
       "PhotoAmt          0.024813\n",
       "adopt             0.019994\n",
       "home              0.016098\n",
       "love              0.013534\n",
       "veri              0.012714\n",
       "pleas             0.011996\n",
       "dog               0.010971\n",
       "play              0.009125\n",
       "wa                0.008203\n",
       "Quantity          0.007895\n",
       "cat               0.007587\n",
       "Mixed             0.007485\n",
       "kitten            0.007485\n",
       "owner             0.007382\n",
       "good              0.007280\n",
       "ha                0.007177\n",
       "care              0.006972\n",
       "FurLength         0.006870\n",
       "hi                0.006357\n",
       "thi               0.006357\n",
       "Fee               0.006357\n",
       "look              0.006254\n",
       "onli              0.005332\n",
       "contact           0.005332\n",
       "MaturitySize      0.005229\n",
       "need              0.005229\n",
       "activ             0.004922\n",
       "month             0.004716\n",
       "...                    ...\n",
       "neighbourhood     0.000103\n",
       "mereka            0.000103\n",
       "anak2             0.000103\n",
       "advic             0.000103\n",
       "affect            0.000103\n",
       "whatsap           0.000103\n",
       "whatapp           0.000103\n",
       "needi             0.000103\n",
       "mark              0.000103\n",
       "adjust            0.000103\n",
       "unit              0.000103\n",
       "molli             0.000103\n",
       "note              0.000103\n",
       "mamak             0.000103\n",
       "mandarin          0.000103\n",
       "marbl             0.000103\n",
       "jojo              0.000103\n",
       "untuk             0.000103\n",
       "ye                0.000103\n",
       "lap               0.000103\n",
       "mana              0.000103\n",
       "mojo              0.000103\n",
       "😊                 0.000103\n",
       "atau              0.000103\n",
       "American          0.000103\n",
       "lain              0.000103\n",
       "unless            0.000103\n",
       "athlet            0.000103\n",
       "noth              0.000103\n",
       "nose              0.000103\n",
       "\n",
       "[1305 rows x 1 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only positive importance makes up 1305\n",
    "nonzero_importance = importance_df[importance_df['importance'] > 0]\n",
    "nonzero_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild Tfidf using only top features\n",
    "I was going to remove the above features from the tfidf vector, but for now will vary the amount arbitarily, based on count. This is perhaps something I can implement later on, when and if I have time to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of all test data descriptions\n",
    "token_dict = {}\n",
    "for idx, desc in df_train['Description'].items():\n",
    "    try:\n",
    "        token_dict[idx] = desc.translate(remove_punc).lower()\n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tfidf for test again, but varying max_features parameter \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english', max_features=10)\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check training features count\n",
    "features = tfidf.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform all data to test data tokens\n",
    "response = tfidf.transform(df_combined['Description'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18941, 10)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of response\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all data to sparse data array and remove test column\n",
    "X_all_sparse = scipy.sparse.csr_matrix(np.array(X_all.drop(columns=['test', 'Description']).values.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy hstack\n",
    "X_sparse_inc_desc = hstack([X_all_sparse, response]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18941, 214)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<18941x214 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 391127 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check new sparse dataset\n",
    "print(X_sparse_inc_desc.shape)\n",
    "X_sparse_inc_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all data into test and training\n",
    "mask_test = np.where(X_all['test'] == 1)[0]\n",
    "mask_train = np.where(X_all['test'] == 0)[0]\n",
    "X_train_sparse = X_all_sparse[mask_train]\n",
    "X_test_sparse  = X_all_sparse[mask_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 4, \n",
    "          'learning_rate': 0.2, \n",
    "          'n_estimators': 200, \n",
    "          'silent': True, \n",
    "          'objective': 'multi:softprob', \n",
    "          'booster': 'gbtree',\n",
    "          'tree_method': 'hist',\n",
    "          'n_jobs': 3,\n",
    "          'gamma': 0, \n",
    "          'min_child_weight': 1, \n",
    "          'max_delta_step': 0, \n",
    "          'subsample': 0.8, \n",
    "          'colsample_bytree': 1, \n",
    "          'colsample_bylevel': 1, \n",
    "          'reg_alpha': 0, \n",
    "          'reg_lambda': 1, \n",
    "          'scale_pos_weight': 1, \n",
    "          'base_score': 0.2, \n",
    "          'random_state': rnd, \n",
    "          'missing': None,\n",
    "          'verbose': 0,\n",
    "          'verbosity': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cv_testing(X_train_all, params, folds=5, dataframe=True):\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    folds = KFold(folds, True, rnd).split(X_train_all)\n",
    "\n",
    "    for train_indx, test_indx in folds:\n",
    "\n",
    "        # flag dataframe determines whether to use iloc or \"normal\" masking for (sparse) arrays\n",
    "        if not dataframe:\n",
    "            X_train, X_test = X_train_all[train_indx], X_train_all[test_indx]\n",
    "            y_train, y_test = y_train_all[train_indx], y_train_all[test_indx]\n",
    "        else:\n",
    "            X_train, X_test = X_train_all.iloc[train_indx], X_train_all.iloc[test_indx]\n",
    "            y_train, y_test = y_train_all.iloc[train_indx], y_train_all.iloc[test_indx]\n",
    "\n",
    "\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        prediction = clf.predict(X_test)\n",
    "\n",
    "        scores.append(ml_metrics.quadratic_weighted_kappa(rater_a=y_test, rater_b=prediction))\n",
    "        print(\"{:.3f}\".format(scores[-1]), end=\"\\t\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:58:48] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.389\t[16:58:50] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.421\t[16:58:53] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.413\t[16:58:56] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.384\t[16:58:58] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.367\t[16:59:01] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.407\t[16:59:04] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.368\t[16:59:06] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.385\t[16:59:09] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.362\t[16:59:12] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "0.364\t\n"
     ]
    }
   ],
   "source": [
    "scores = cv_testing(X_train_all=X_train_sparse, folds=10, params=params, dataframe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38597978973077507"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.386 --> 0.390 (changed tree method to 'hist')\n",
    "# 0.390 --> 0.374 (tfidf features 10)\n",
    "# 0.390 --> 0.366 (tfidf features 100)\n",
    "# 0.390 --> 0.370 (tfidf features 100, using training data corpus)\n",
    "# 0.390 --> 0.374 (tfidf features 10, using training data corpus)\n",
    "# 0.390 --> 0.386 (no tfidf?! note: also no keyword flags)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of adding tfidf features appears to be backwards. This could be a result of multiple factors:\n",
    " - Made a mistake somewhere\n",
    " - Tfidf is not suitable\n",
    " - Model is overfitting during training\n",
    "\n",
    "Will clean up and reassess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(**params)\n",
    "clf.fit(X_train_all, y_train_all)\n",
    "prediction = clf.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'AdoptionSpeed': prediction.astype(int)}, index=X_test_all.index)\n",
    "submission.to_csv(\"submission.csv\", index=True, index_label='PetID', header=['AdoptionSpeed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
